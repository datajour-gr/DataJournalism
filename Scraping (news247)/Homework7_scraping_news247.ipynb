{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework7 - scraping_news247.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "t47Nt5j4uAF7",
        "-ydGxCZQxNdA",
        "eTXDR1CZxZxf",
        "dZRGR_V8ILUG",
        "35BXWvCDgTDF"
      ],
      "authorship_tag": "ABX9TyNr5mDKmdcvWWln14OlC2kM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tsparaskevas/DataJournalism/blob/main/Scraping%20(news247)/Homework7_scraping_news247.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Μεθοδολογία κατασκευής scrapper για όλα τα άρθρα ενός site (με requests και BeautifulSoup) - [news247.gr]"
      ],
      "metadata": {
        "id": "fBJC8PgMkJ4w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Στόχος**: \n",
        "\n",
        "###Να μαζέψω όλα τα δεδομένα όλων των τελευταίων άρθρων που εμφανίζονται στις 5 πρώτες σελίδες της ροής ειδήσεων του news247.gr.\n",
        "\n",
        "Δεν θέλω απλώς το περιεχόμενο των teaser articles (δηλαδή μόνο ένα ελάχιστο τμήμα της αρχής της είδησης που συνήθως εμφανίζεται στις σελίδες της ροής των ειδήσεων), αλλά και ολόκληρο το σώμα κειμένου του κάθε άρθρου.\n",
        "\n",
        "Αυτό σημαίνει ότι πρέπει να μπω στη σελίδα που εμφανίζεται ολόκληρο το κάθε άρθρο και να μαζέψω από εκεί όλη την πληροφορία.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "nP091HVMjJJa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Τι χρειάζομαι**:\n",
        "\n",
        "1. να μαζέψω όλα τα links (urls) των σελίδων που δημοσιεύονται ολόκληρα τα άρθρα\n",
        "2. να δω τη δομή των σελίδων ώστε να επιλέξω τα κατάλληλα tags και attributes για την κάθε πληροφορία που με ενδιαφέρει."
      ],
      "metadata": {
        "id": "5QBmgwK5Rpd4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Πώς θα το κάνω**:\n",
        "\n",
        "Πριν αρχίσω να γράφω κώδικα, πρέπει να οργανώσω τα βήματα που θα δουλέψω:\n",
        "\n",
        "1. Για το μάζεμα των links:\n",
        "  * αφού θέλω τα άρθρα των 5 πρώτων σελίδων της ροής ειδήσεων (https://www.news247.gr/latest/), πρέπει να δω ποιο είναι το url τους. Στην παραπάνω διεύθυνση βρίσκεται η πρώτη σελίδα της ροής ειδήσεων, αλλά εγώ θέλω και τις 4 επόμενες. \n",
        "  * αφού θέλω την πληροφορία ολόκληρων των άρθρων, πρέπει να έχω τα url τους. Τo url του ολόκληρου άρθρου (όπως έχουμε δει και από το bbc) υπάρχει στο href του `<a>` tag στο αντίστοιχο teaser article.\n",
        "\n",
        "Επομένως, θέλω τα urls των 5 πρώτων σελίδων της ροής ειδήσεων και από κάθε μια απ' αυτές τις σελίδες, θέλω τα urls των teaser articles που περιέχει.\n",
        "\n",
        "2. Για το μάζεμα των πληροφοριών από το κάθε άρθρο:\n",
        "  * όπως κάναμε και με το bbc, πρέπει να κάνω inspect σε μια σελίδα ολόκληρου άρθρου και να δω πώς είναι οργανωμένη η πληροφορία (δηλαδή, από ποιο tag, με ποιο class αν χρειάζεται και ποιο attribute) ώστε να κάνω τα σωστά find_all και find.\n",
        "\n",
        "3. Όπως και στο παράδειγμα του bbc, θα φτιάξω μια λίστα με dictionaries που θα περιέχουν τα δεδομένα για κάθε άρθρο και τέλος θα μετατρέψω αυτή τη λίστα σε dataframe."
      ],
      "metadata": {
        "id": "9w9i893AGHgp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Bήμα 1**. Πώς εμφανίζεται η ροή των ειδήσεων στο news247;"
      ],
      "metadata": {
        "id": "lThkiA27NavA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Μπαίνω στη σελίδα https://www.news247.gr/latest/ και βλέπω πώς εμφανίζονται τα άρθρα.\n",
        "\n",
        "Κρατάω εδώ σημειώσεις σχετικά με:\n",
        "\n",
        "* Πόσα άρθρα περιέχει η σελίδα:\n",
        "* Πώς εμφανίζονται περισσότερα (δηλαδή τα παλαιότερα) άρθρα:\n",
        "* Τι αλλάζει στο url όταν προχωράω σε επόμενες σελίδες:\n",
        "* Ποιο είναι το βασικό url:\n",
        "* Τι πρέπει να συμπληρώνω στο url για να πηγαίνω (με python) σε επόμενη σελίδα:\n"
      ],
      "metadata": {
        "id": "o7-kuJEna75M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Βήμα 2**. Κατέβασμα των 5 πρώτων σελίδων από τις \"Ειδήσεις\" του news247"
      ],
      "metadata": {
        "id": "5gxEDsSPcGkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Εισαγωγή βιβλιοθηκών (requests, BeautifulSoup, pandas και time)\n"
      ],
      "metadata": {
        "id": "HIdFt3nZcc2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Κατέβασμα των 5 πρώτων σελίδων και αποθήκευσή τους σε μια λίστα\n",
        "\n",
        "# Φτιάχνω μεταβλητή με το βασικό url\n",
        "main_url = \n",
        "# Φτιάχνω μια κενή λίστα για την αποθήκευση όλων των html των 5 πρώτων σελίδων\n",
        "\n",
        "# Φτιάχνω μια for loop, η οποία θα:\n",
        "for i in range(1, 6) # μας δίνει κάθε φορά έναν αριθμό από το 1 έως το 5 (στο range βάζουμε για αρχή τον αριθμό που θέλουμε να αρχίσει και για τέλος τον αριθμο που θέλουμε να τελειώσει +1) \n",
        "  latest_page_url = main_url + str(i) # σχηματίζει ολόκληρο το url για την κάθε μία από τις 5 σελίδες που θέλω (βάζουμε str(i) γιατι το i είναι αριθμός και το main_url string)\n",
        "                                 # κατεβάζει τη σελίδα (requests)\n",
        "                                 # μετατρέπει την κατεβασμένη σελίδα σε διαχειρίσιμη html (BeautifulSoup)\n",
        "                                 # αποθηκεύει την κάθε σελίδα που κατεβαίνει στη λίστα με όλες τις σελίδες που φτιάξαμε στην αρχή (append)\n",
        "                                 # σταματάει για 2 δευτερόλεπτα για προστασία του server (sleep(2))"
      ],
      "metadata": {
        "id": "MeBL3fN1ci0b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "p2Pd2wxHhasH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Βήμα 3**. Μάζεμα των links των teaser articles από όλες τις σελίδες που κατέβασα σε μια λίστα"
      ],
      "metadata": {
        "id": "i2F19hs5gUko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Φτιάχνω μια λίστα που θα αποθηκεύσω όλα τα links των ολόκληρων άρθρων που θα πάρω από τα teaser articles\n",
        "\n"
      ],
      "metadata": {
        "id": "TCECls_d82EG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "hU3m-RK0hZ73"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Βήμα 4**. Κατέβασμα των html όλων των σελίδων των ολόκληρων άρθρων"
      ],
      "metadata": {
        "id": "hIXuN2iOhZNn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Φτιάχνω μια κενη λίστα που θα αποθηκεύσω όλες τις html όλων των ολόκληρων άρθρων που θα κατεβάσω\n",
        "\n",
        "# Φτιάχνω μια for loop, η οποία παίρνει ένα ένα τα links των ολόκληρων άρθρων από τη λίστα που έφτιαξα στο Βήμα 3 και κατεβάζει την html τους"
      ],
      "metadata": {
        "id": "oJU6zqHskblq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Βήμα 5**. Κοιτάζω με το inspect πώς είναι δομημένο το περιεχόμενο της σελίδας του πλήρους άρθρου\n",
        "(ώστε να αναζητήσω με τα κατάλληλα tags τα δεδομένα που θέλω για το κάθε άρθρο)\n",
        "\n",
        "Γράφω εδώ τα ευρήματά μου:\n",
        "* **Title**: ('div', {'class': 'article-body '}).find('h1').text\n",
        "* **Lead**: \n",
        "* **Section tag**:\n",
        "* **Picture alt text**: (προαιρετικά μπορείτε να πάρετε και το alt text της κύριας φωτογραφίας)\n",
        "* **Picture url**: (επίσης προαιρετικά μπορείτε να πάρετε και το url της κύριας φωτογραφίας)\n",
        "* **Editor**: \n",
        "* **Date**: \n",
        "* **Body**:"
      ],
      "metadata": {
        "id": "-ydGxCZQxNdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Παρατήρηση 1**:\n",
        "\n",
        "Όπως φαίνεται στο άρθρο στον browser (https://www.news247.gr/kosmos/eylogia-ton-pithikon-proto-epivevaiomeno-kroysma-kai-sti-gallia.9637679.html), ανάμεσα στις παραγράφους, υπάρχει κι ένας υπότιτλος (`<h2>`). Αν τον πάρω μόνο του δεν θα τον έχω μέσα στη ροή του κειμένου.\n",
        "\n",
        "Mπορώ όμως με το find_all, να ζητήσω ταυτόχρονα τα `<p>` και τα `<h2>`\n",
        "\n",
        "```\n",
        "find('div', {'class': 'article-body__body'}).find_all(['p', 'h2'])\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "pf6q4P5CApmQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Παρατήρηση 2**:\n",
        "\n",
        "Κάνοντας find_all τα p και τα h2, μπορώ να έχω σε μια λίστα όλες τις παραγράφους και τους υπότιτλους. Αφού εγώ θέλω όλο το body σαν ένα string, μπορώ να συνενώσω όλα τα επιμέρους strings (δηλαδή τις επιμέρους παραγράφους και υπότιτλους).\n",
        "\n",
        "Παράδειγμα:\n",
        "```\n",
        "# Μάζεμα του text όλων των p και h2 σε μία λίστα\n",
        "body = []\n",
        "for p in soup.find('div', {'class': 'article-body__storyContent '}).find_all(['p', 'h2']):\n",
        "  body_text = p.text\n",
        "  body.append(body_text)\n",
        "\n",
        "# Συνένωση των μεμονωμένων στοιχείων του κειμένου σε ένα string και μετατροπή του τυχόν διπλού κενού που μπορεί να δημιουργείται σε κάποιες περιπτώσεις σε μονό\n",
        "full_body_text = ' '.join(body.replace(\"  \", \" \")\n",
        "```\n"
      ],
      "metadata": {
        "id": "gMsAOifOrKzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Βήμα 6**. Συλλογή όλων των δεδομένων των άρθρων (Τίτλος, section, lead, body κλπ.) και αποθήκευσή τους σε μια λίστα με dictionaries."
      ],
      "metadata": {
        "id": "dZRGR_V8ILUG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gtDiQxj8vNX3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "_XI8MUb7vNPg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "gRIKN33yvNF4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "####**Βήμα 7**: Μετατροπή της λίστας με τα dictionaries σε dataframe."
      ],
      "metadata": {
        "id": "pTheiS89vMNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "D0P4HGepDB4z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "-YoYIyHwSUTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "tgCOEm3BYaWr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}